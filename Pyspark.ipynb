{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando Mount entre o Azure Storage Account com o Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mount (source = \"wasbs://conteiner1@storageprojeto1.blob.core.windows.net\",\n",
    "                 mount_point = \"/mnt/blobdataset\",\n",
    "               extra_configs = {'fs.azure.account.key.storageprojeto1.blob.core.windows.net': dbutils.secrets.get ('mount_Scope','chaveBlob')} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leitura do arquivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv ('/mnt/blobdataset/dataset_transicoes.csv' ,header=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script de processamento dos dados e filtragem dos clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "df = spark.read.csv ('/mnt/blobdataset/dataset_transicoes.csv' ,header=True)\n",
    "\n",
    "#Alteração nome da coluna do dataset e convertendo a coluna \"Valor\" de string para numerico\n",
    "df = df.withColumnRenamed(\"nome\", \"Nome_Cliente\")\n",
    "df = df.withColumn(\"Valor\", df[\"Valor\"].cast(\"double\"))\n",
    "\n",
    "\n",
    "#Filtrando os clientes que tem o maior limite e que tiveram transições acima de 4000 reais\n",
    "clientes = df.filter((col(\"limite_credito\") > 5000) & (col(\"valor\") > 4000))\n",
    "resultado = clientes.select(\"CPF\", \"Nome_Cliente\", \"Limite_Credito\", \"Valor\")\n",
    "\n",
    "#Agrupando as colunas do dataset e usando a função sum para somar os valores da coluna \"Valor\"\n",
    "df_resultado_final = df.groupBy(\"CPF\", \"Nome_Cliente\", \"Limite_Credito\").agg(sum(\"Valor\").alias(\"Valor_Total\"))\n",
    "\n",
    "#Removendo duplicação e agrupando por Nome do cliente\n",
    "df_resultado_final = df_resultado_final.distinct()\n",
    "df_resultado_final = df_resultado_final.orderBy(\"Nome_Cliente\")\n",
    "\n",
    "\n",
    "display(df_resultado_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexão do Databricks com o Azure SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexão do Azure databricks com Azure SQL Database\n",
    "\n",
    "server_name = \"servidor-projeto.database.windows.net\"\n",
    "database_name = \"Database_Projeto\"\n",
    "username = \"projeto_integrador\"\n",
    "password = \"teste@5321\"\n",
    "\n",
    "# JDBC URL para conexão com o Azure SQL Database\n",
    "jdbc_url = f\"jdbc:sqlserver://{server_name}:1433;database={database_name};user={username};password={password};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
    "\n",
    "\n",
    "# Escrevendo o DataFrame no Azure SQL Database\n",
    "df_resultado_final.write.jdbc(url=jdbc_url, table=\"Clientes\", mode=\"overwrite\", properties={\"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"})\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
